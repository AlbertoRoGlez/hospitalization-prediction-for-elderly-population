{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = pd.read_csv(\"data/waves_norm/wave1_norm.csv\")\n",
    "w2 = pd.read_csv(\"data/waves_norm/wave2_norm.csv\")\n",
    "w3 = pd.read_csv(\"data/waves_norm/wave3_norm.csv\")\n",
    "w4 = pd.read_csv(\"data/waves_norm/wave4_norm.csv\")\n",
    "w5 = pd.read_csv(\"data/waves_norm/wave5_norm.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26839, 370)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r1tr8_m</th>\n",
       "      <th>r1dresshlp</th>\n",
       "      <th>r1quitsmok</th>\n",
       "      <th>r1rorgnz</th>\n",
       "      <th>r1walkra</th>\n",
       "      <th>r1cncrmeds</th>\n",
       "      <th>r1oopmd1y</th>\n",
       "      <th>r1livsib</th>\n",
       "      <th>r1rarcarehr</th>\n",
       "      <th>r1stroklmt</th>\n",
       "      <th>...</th>\n",
       "      <th>r1walkhlp</th>\n",
       "      <th>r1iothr</th>\n",
       "      <th>r1rpfcaredpm</th>\n",
       "      <th>r1lost</th>\n",
       "      <th>r1iadlfourm</th>\n",
       "      <th>r1adla</th>\n",
       "      <th>r1doctim1y</th>\n",
       "      <th>r1rfcaredpmm</th>\n",
       "      <th>r1iqscore9</th>\n",
       "      <th>r1paina</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.333334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.666666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 370 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     r1tr8_m  r1dresshlp  r1quitsmok  r1rorgnz  r1walkra  r1cncrmeds  \\\n",
       "0  10.333333         NaN        15.0       NaN       0.0         0.0   \n",
       "1  13.333334         NaN         NaN       NaN       0.0         0.0   \n",
       "2  13.000000         NaN         NaN       NaN       0.0         0.0   \n",
       "3  10.666666         NaN         NaN       NaN       0.0         0.0   \n",
       "4   8.000000         NaN         3.0       NaN       0.0         0.0   \n",
       "\n",
       "   r1oopmd1y  r1livsib  r1rarcarehr  r1stroklmt  ...  r1walkhlp  r1iothr  \\\n",
       "0      775.0       2.0          NaN         NaN  ...        NaN      0.0   \n",
       "1      500.0       5.0          NaN         NaN  ...        NaN      0.0   \n",
       "2      380.0      16.0          NaN         NaN  ...        NaN      0.0   \n",
       "3        0.0       6.0          NaN         NaN  ...        NaN      0.0   \n",
       "4     2250.0       9.0          NaN         NaN  ...        NaN      0.0   \n",
       "\n",
       "   r1rpfcaredpm  r1lost  r1iadlfourm  r1adla  r1doctim1y  r1rfcaredpmm  \\\n",
       "0           NaN     NaN          0.0     0.0         4.0           NaN   \n",
       "1           NaN     NaN          0.0     0.0         1.0           NaN   \n",
       "2           NaN     NaN          0.0     0.0         2.0           NaN   \n",
       "3           NaN     NaN          0.0     0.0         0.0           NaN   \n",
       "4           NaN     NaN          0.0     0.0         0.0           NaN   \n",
       "\n",
       "   r1iqscore9  r1paina  \n",
       "0         NaN      0.0  \n",
       "1         NaN      NaN  \n",
       "2         NaN      1.0  \n",
       "3         NaN      NaN  \n",
       "4         NaN      NaN  \n",
       "\n",
       "[5 rows x 370 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(word:str):\n",
    "    print(word.lower())\n",
    "\n",
    "def unique_values(database ,label:str):\n",
    "    \n",
    "    label = label.lower()\n",
    "    n_obs = database.shape[0]\n",
    "    nunique_label = database[label].nunique()\n",
    "\n",
    "    print(f\"n° unique values is {nunique_label} of {n_obs}\")\n",
    "\n",
    "def clean_cardinality(df):\n",
    "    \n",
    "    \"\"\"Drops all columns with high and low cardinality\"\"\"\n",
    "    n_unique_values = df.nunique()\n",
    "\n",
    "    high = n_unique_values[n_unique_values == df.shape[0]].index\n",
    "    low = n_unique_values[n_unique_values == 1 ].index\n",
    "    total = [*high, *low]\n",
    "\n",
    "    df.drop(columns=total, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_missing(df, threshold=0.3):\n",
    "    \n",
    "    \"\"\"drops all columns above a certain threshold\"\"\"\n",
    "\n",
    "    # mysterious treatment.\n",
    "    #df[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "    # now, my treatment\n",
    "    nulls = df.isnull().sum()/df.shape[0]\n",
    "    #nulls.sort_values(ascending=False, inplace=True)\n",
    "    null_columns = list(nulls[nulls>=threshold].index)\n",
    "    df.drop(columns = null_columns, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_correlated_features(X, threshold=0.5):\n",
    "\n",
    "    \"\"\"Find correlated columns in a DataFrame and drop them \n",
    "    \n",
    "    Arguments:\n",
    "        df (DataFrame): Data to analize.\n",
    "        threshold (float): Minimun correlation value considered to decide whether\n",
    "        two columns are correlated or not.\n",
    "     \n",
    "    Rreturns:\n",
    "        A list with non-correlated columns.\"\"\"\n",
    "    \n",
    "    numeric_features = list(X.select_dtypes(include=['int', 'float']).columns)\n",
    "    numeric_data= X[numeric_features].copy()\n",
    "    # Creating correlation matrix and getting their absolute values.\n",
    "    corr = numeric_data.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape, dtype=bool), k=1))\n",
    "    # Find features with correlation greater than 0.95\n",
    "    the_drop = [column for column in upper.columns if any(upper[column] >= threshold)]\n",
    "\n",
    "    no_correlated_columns = list(numeric_data.drop(the_drop, axis=1).columns)\n",
    "\n",
    "    return no_correlated_columns\n",
    "\n",
    "def get_features(X, type:str, get=\"all\"):\n",
    "\n",
    "    \"\"\"Extract categorical or numeric features from a DataFrame\n",
    "\n",
    "    Arguments:\n",
    "        df (dataframe): Data to analize\n",
    "        type (str): {\"c, \"n\"} Whether the desired features is categorical(c)\n",
    "                    or numeric(n).\n",
    "        get (str): Whether extract only binary, no binary or all \n",
    "        categorical features {\"all\", \"binary\", \"no_binary\"}\n",
    "        \n",
    "    Return:\n",
    "     list of all, binary or no binary categorical features.\n",
    "    \"\"\"\n",
    "    if type==\"c\":\n",
    "        # getting a table with only categorical features an their n° of unique values.\n",
    "        cat_feat = X.select_dtypes(include=['O']).nunique()\n",
    "\n",
    "        # from the object features: filtering the binary features.\n",
    "        bin_cat_feat = cat_feat[cat_feat == 2].index\n",
    "        # from the object features: filtering the non-binary features.\n",
    "        no_bin_cat_feat = cat_feat[cat_feat != 2].index\n",
    "\n",
    "        if get==\"all\":\n",
    "            col = list(cat_feat.index)\n",
    "        elif get==\"binary\":\n",
    "            col= list(bin_cat_feat)\n",
    "        elif get==\"no_binary\":\n",
    "            col=list(no_bin_cat_feat)\n",
    "        else:\n",
    "            raise Exception(\"'get' must be in {all, binary, no_binary}\")\n",
    "    elif type ==\"n\":\n",
    "        # getting the names of the numerical features\n",
    "        num_feat = X.select_dtypes(include=['int', 'float']).nunique()\n",
    "\n",
    "        if get==\"all\":\n",
    "            col = list(num_feat.index)\n",
    "        elif get==\"binary\":\n",
    "            bin_num_feat = num_feat[num_feat==2].index\n",
    "            col= list(bin_num_feat)\n",
    "        elif get==\"no_binary\":\n",
    "            no_bin_num_feat = num_feat[num_feat!=2].index\n",
    "            col=list(no_bin_num_feat)\n",
    "        else:\n",
    "            raise Exception(\"'get' must be in {all, binary, no_binary}\")\n",
    "    else:\n",
    "        raise Exception(\"'type' must be in {c, n}\")\n",
    "    \n",
    "    return col\n",
    "\n",
    "\n",
    "def val_col(df, columns_to_select):\n",
    "    valid_columns = [column for column in columns_to_select if column in df.columns]\n",
    "    return valid_columns\n",
    "\n",
    "def sel(df, columns_to_select):\n",
    "    col = val_col(df, columns_to_select)\n",
    "    return df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n° features original data: 370\n",
      "n° features after drop_correlated: 121\n",
      "n° features after clean_missing & clean_cardinality: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"n° features original data: {w1.shape[1]}\")\n",
    "no_corr_columns = drop_correlated_features(w1, threshold=0.4)\n",
    "w1_no_correlated = w1[no_corr_columns].copy()\n",
    "print(f\"n° features after drop_correlated: {w1_no_correlated.shape[1]}\")\n",
    "w1_clean= clean_cardinality(w1_no_correlated)\n",
    "#w1_clean = w1_no_correlated[w1_clean_columns].copy()\n",
    "print(f\"n° features after clean_missing & clean_cardinality: {w1_clean.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15150, 95)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_clean.dropna(subset=[\"r1hosp1y\"], inplace=True)\n",
    "w1_clean.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r1hosp1y       1.000000\n",
       "r1oophosf1y    0.263367\n",
       "r1oopmd1y      0.203817\n",
       "r1dresshlp     0.186922\n",
       "r1doctor1y     0.166849\n",
       "                 ...   \n",
       "r1smoken      -0.037122\n",
       "r1wthh        -0.041767\n",
       "r1rarcarehr   -0.052259\n",
       "r1alone       -0.141456\n",
       "r1iwstat            NaN\n",
       "Name: r1hosp1y, Length: 95, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1_clean.corr()[\"r1hosp1y\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "target = \"r1hosp1y\"\n",
    "X = w1_clean.drop(columns=target).copy()\n",
    "y = w1_clean[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m X_smote, y_smote \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n\u001b[0;32m      4\u001b[0m df_smote \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_smote, y_smote], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\AnyoneAI\\final-project\\env\\Lib\\site-packages\\imblearn\\base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[1;32m~\\Documents\\AnyoneAI\\final-project\\env\\Lib\\site-packages\\imblearn\\base.py:82\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     80\u001b[0m check_classification_targets(y)\n\u001b[0;32m     81\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m---> 82\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     88\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32m~\\Documents\\AnyoneAI\\final-project\\env\\Lib\\site-packages\\imblearn\\base.py:156\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    154\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    155\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 156\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32m~\\Documents\\AnyoneAI\\final-project\\env\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\Documents\\AnyoneAI\\final-project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\Documents\\AnyoneAI\\final-project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\Documents\\AnyoneAI\\final-project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "df_smote = pd.concat([X_smote, y_smote], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bin = get_features(X_train, \"n\", 'binary')\n",
    "#X_train_num_bin = X_train[num_bin]\n",
    "\n",
    "num_no_bin = get_features(X_train, \"n\", 'no_binary')\n",
    "#X_train_num_no_bin = X_train[num_no_bin]\n",
    "\n",
    "################# nothing here ######################\n",
    "X_train_cat_bin = get_features(X_train, \"c\", 'binary') # nothing\n",
    "X_train_cat_no_bin = get_features(X_train, \"c\", 'no_binary') # nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, power_transform\n",
    "\n",
    "def preprocess_no_bin(X_train, X_test):\n",
    "\n",
    "    si = SimpleImputer(strategy='median')\n",
    "    si.fit(X_test)\n",
    "    X_test = si.transform(X_test)\n",
    "    X_train = si.transform(X_train)\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_test)\n",
    "    X_test=ss.transform(X_test)\n",
    "    X_train=ss.transform(X_train)\n",
    "\n",
    "    X_test = power_transform(X_test, method='yeo-johnson')\n",
    "    X_train = power_transform(X_train, method='yeo-johnson')\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def preprocess_bin(X_train, X_test):\n",
    "\n",
    "    si = SimpleImputer(strategy='most_frequent')\n",
    "    si.fit(X_test)\n",
    "    X_test = si.transform(X_test)\n",
    "    X_train = si.transform(X_train)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "X_train[num_no_bin], X_test[num_no_bin] = preprocess_no_bin(X_train[num_no_bin], X_test[num_no_bin])\n",
    "X_train[num_bin], X_test[num_bin] = preprocess_bin(X_train[num_bin], X_test[num_bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "\n",
    "log = LogisticRegression(random_state=42, \n",
    "                         max_iter=1000, \n",
    "                         n_jobs=-1, \n",
    "                         class_weight={0:0.1, 1:0.9})\n",
    "log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "y_pred = log.predict(X_test)\n",
    "\n",
    "roc_auc_train = roc_auc_score(y_train, log.predict(X_train))\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val = roc_auc_score(y_test, y_pred)\n",
    "print(f\"roc_auc for train data: {roc_auc_train}\", \n",
    "      f\"roc_auc for test data: {roc_auc_val}\",\n",
    "      sep='\\n')\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(confusion_matrix(y_test, y_pred),\n",
    "    classification_report(y_test, y_pred),\n",
    "    sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('fivethirtyeight'):\n",
    "    sns.histplot(log.predict_proba(X_test), bins=100)\n",
    "    plt.title(\"Probability distribution for each Target category\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df, columns):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    \n",
    "    num_imputer = KNNImputer(n_neighbors=2)\n",
    "    \n",
    "    imputer = ColumnTransformer([(\"num_imputer\",num_imputer,columns)])\n",
    "    \n",
    "    filled_columns = imputer.fit_transform(df)\n",
    "    df_filled = pd.DataFrame(filled_columns, columns=columns)\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# columns_to_impute = w5.columns.tolist()\n",
    "# w5_filled = impute_missing_values(w5, columns_to_impute)\n",
    "# w5_filled.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc_auc for train data: 0.7617286962427524\n",
    "roc_auc for test data: 0.7155879264505463\n",
    "[[2109  608]\n",
    " [ 108  205]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.95      0.78      0.85      2717\n",
    "         1.0       0.25      0.65      0.36       313\n",
    "\n",
    "    accuracy                           0.76      3030\n",
    "   macro avg       0.60      0.72      0.61      3030\n",
    "weighted avg       0.88      0.76      0.80      3030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
