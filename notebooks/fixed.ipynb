{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Current directory: c:\\Users\\Leo\\OneDrive\\Escritorio\\Final Proyect\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import __confing\n",
    "__confing.change_to_root_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src import value_filler as vf\n",
    "from src import feature_selection as fs\n",
    "from src import data_procces as dp\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from src import evaluation\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error,roc_auc_score,confusion_matrix\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, df:pd.DataFrame, target:str, n:int):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    feature_names_original = list(df.columns)\n",
    "    feature_names_original.remove(target)\n",
    "    \n",
    "    max_importance = list(indices[:n])\n",
    "    \n",
    "    nueva_lista = [feature_names_original[i] for i in max_importance]\n",
    "    \n",
    "    return nueva_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, model.predict(X_train))\n",
    "    # Validation ROC AUC Score\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc_auc_val = roc_auc_score(y_test, y_pred)\n",
    "    print( f\"Model Accuracy: {acc}\",\n",
    "            f\"Train roc_auc: {roc_auc_train}\", \n",
    "            f\"Test roc_auc: {roc_auc_val}\",\n",
    "            sep='\\n', end='\\n\\n')\n",
    "    print(confusion_matrix(y_test, y_pred),\n",
    "    classification_report(y_test, y_pred),\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "n='a'\n",
    "df=pd.read_csv(r'data\\custom\\all_waves.csv')\n",
    "target=f'p{n}hosp1y'\n",
    "drop=[f'p{n}hspnit1y','paoophos1y','paoophosf1y'] # si la wave3 falla quitar estas columnas , 'r3bpsft', 'r3bpref'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "df = df.drop(drop, axis=1)\n",
    "\n",
    "df = df.dropna(subset=[target]) \n",
    "\n",
    "porcentaje_nulos = df.isnull().sum(axis=1) / len(df.columns)\n",
    "df = df[(df[target] != 0) | (porcentaje_nulos <= 0.4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill\n",
    "y = df[target]\n",
    "X = df.drop(target,axis=1)\n",
    "X = fs.fast_fill_2(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "feature_selected=['padoctor1y','paurina2y','pacholst','pabreast','paprost','papapsm','pamammog','pacage','padrinkcr','padrinkbd','padrinkb','paheight','paweight','pabmi','papaina','papainlv','papainfr','pafatigue','pawheeze','pabreath_m','paswell','pahearaid','pahearing','pasight','paglasses','palunglmt_m','pahrtatlmt','pastroklmt','paarthlmt','palowermoba','pauppermoba','pagrossaa','palgmusaa','pamobilaa','paclims','pasit','paarms','palift','pastoop','pachair','pameals','pashop','pamoney','pameds','pabedhlp','pabed','patoilt','paeat','pabath','pawalkr','padress','pahipcomp','pafall','pafallinj','pamhip','pahipe_m','parxarthr','parxstrok','parxhrtat','pacncrothr','pacncrmeds','pacncrradn','pacncrsurg','pacncrchem','parxdiab','parxdiabo','parxhibp','parechrtatt','parifaany','parfaany','parfcaren','pararcare','padresshlp', 'pawalkhlp', 'pabathehlp', 'paeathlp', 'pabedhlp', 'patoilethlp', 'pamealhlp', 'pashophlp', 'pamedhlp', 'pamoneyhlp','pahibpe','pagender','padiabe','pacancre','palunglmt_m','parxlung_m','pahrtatte','pastroke','paarthre','parifaany','parafaany','pahigov','pasmokev','pasmoken','paoangry','paosleep','paodngr','paodngr','paopace','paoplot','paoalchl','pawthh','paagey','pamomage','padadage','paprmem','parjudg','parorgnz']\n",
    "X=X[feature_selected]\n",
    "print(len(feature_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "X_train, X_test, y_train, y_test=dp.split_data(X, y, test_size=0.2, random_state=40)\n",
    "X_test_o, y_test_o = X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample \n",
    "X_train, y_train = dp.apply_resample(X_train, y_train,v=1.20)\n",
    "X_test, y_test = dp.apply_resample(X_test, y_test,v=1.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote\n",
    "X_train, y_train = dp.apply_smote(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "X_train, X_test = dp.apply_standard_scaler(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC=0.8121562797082233\n",
      "pahosp1y\n",
      "0.0    3502\n",
      "1.0    2919\n",
      "Name: count, dtype: int64\n",
      "\n",
      "######################################################\n",
      "\n",
      "AUC=0.502674345121568\n",
      "pahosp1y\n",
      "0.0    21954\n",
      "1.0     2919\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# raise ValueError('comentar el error')\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_xgboost_with_kfold(features, labels, model):\n",
    "    k = 10\n",
    "    kf = KFold(n_splits=k)\n",
    "    \n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "        # Entrenar el modelo XGBoost\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_k = xgb.XGBClassifier()\n",
    "model_k= train_xgboost_with_kfold(X_train, y_train,model_k)\n",
    "\n",
    "y_pred_proba = model_k.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'AUC={auc}')\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print('')\n",
    "print('######################################################')\n",
    "print('')\n",
    "\n",
    "y_pred_proba = model_k.predict_proba(X_test_o)[:, 1]\n",
    "auc = roc_auc_score(y_test_o, y_pred_proba)\n",
    "print(f'AUC={auc}')\n",
    "print(y_test_o.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "comentar el error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[413], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcomentar el error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moptuna\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m KFold\n",
      "\u001b[1;31mValueError\u001b[0m: comentar el error"
     ]
    }
   ],
   "source": [
    "raise ValueError('comentar el error')\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Definir los hiperpar√°metros a optimizar\n",
    "    params = {\n",
    "        'booster': 'gbtree',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 1.0),\n",
    "        \n",
    "        'max_depth': trial.suggest_int('max_depth', 12, 35),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 1.0),\n",
    "    }\n",
    "    \n",
    "    k = 10\n",
    "    kf = KFold(n_splits=k)\n",
    "    \n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test_o)[:, 1]\n",
    "    auc = roc_auc_score(y_test_o, y_pred_proba)\n",
    "    return auc\n",
    "\n",
    "features = X_train\n",
    "labels = y_train\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('comentar el error')\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Definir los hiperpar√°metros a optimizar\n",
    "    params = {\n",
    "        'booster': 'dart',\n",
    "        # 'eta': trial.suggest_float('eta', 0.01, 0.1),\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1.0),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 1.0),\n",
    "        \n",
    "        'max_depth': trial.suggest_int('max_depth', 12, 30),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 1.0),\n",
    "    }\n",
    "    \n",
    "    k = 10\n",
    "    kf = KFold(n_splits=k)\n",
    "    \n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test_o)[:, 1]\n",
    "    auc = roc_auc_score(y_test_o, y_pred_proba)\n",
    "    return auc\n",
    "\n",
    "features = X_train\n",
    "labels = y_train\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise ValueError('comentar el error')\n",
    "feature_selected = feature_importance(model_k, df, target, n=70)\n",
    "print(feature_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'eta': 0.017278777004832203, 'reg_alpha': 0.5675043688405764, 'reg_lambda': 0.4793430182530139, 'max_depth': 17, 'learning_rate': 0.007819974793975336, 'n_estimators': 935, 'subsample': 0.7967742160565803, 'colsample_bytree': 0.302428450643778}\n",
    "{'eta': 0.02475425607581313, 'reg_alpha': 0.858108471588533, 'reg_lambda': 0.6377242839694902, 'max_depth': 22, 'learning_rate': 0.009207206229839365, 'n_estimators': 917, 'subsample': 0.7791266864861979, 'colsample_bytree': 0.27198345018350145}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
