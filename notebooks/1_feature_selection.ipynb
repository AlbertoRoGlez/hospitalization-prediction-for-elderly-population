{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\Leo\\OneDrive\\Escritorio\\Final Proyect\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import __confing\n",
    "__confing.change_to_root_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    __confing.execute_notebook('notebooks/data_extract.ipynb')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from src import data_procces as dp\n",
    "from src.feature_selection import fast_fill\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, X: pd.DataFrame, y: pd.Series, n: int):\n",
    "    \"\"\"\n",
    "    Obtain the top 'n' feature importance values from a given model.\n",
    "    \n",
    "    Args:\n",
    "        model (object): The trained model used for feature importance calculation.\n",
    "        X (pd.DataFrame): The input dataframe containing the features.\n",
    "        y (pd.Series): The target variable series.\n",
    "        n (int): The number of top feature importances to return.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of the top 'n' feature names with the highest importances.\n",
    "    \"\"\"\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    feature_names_original = list(X.columns)\n",
    "    \n",
    "    target_name = y.name  # Get the name of the target column from the Series y\n",
    "    \n",
    "    if target_name in feature_names_original:\n",
    "        feature_names_original.remove(target_name)\n",
    "    \n",
    "    max_importance = list(indices[:n])\n",
    "    \n",
    "    nueva_lista = [feature_names_original[i] for i in max_importance]\n",
    "    \n",
    "    return nueva_lista\n",
    "\n",
    "\n",
    "\n",
    "def drop_correlated_features(X, threshold=0.5):\n",
    "    \"\"\"Find correlated columns in a DataFrame and drop them \n",
    "    \n",
    "    Arguments:\n",
    "        df (DataFrame): Data to analize.\n",
    "        threshold (float): Minimun correlation value considered to decide whether\n",
    "        two columns are correlated or not.\n",
    "    \n",
    "    Rreturns:\n",
    "        A list with non-correlated columns.\"\"\"\n",
    "    \n",
    "    numeric_features = list(X.select_dtypes(include=['int', 'float']).columns)\n",
    "    random.shuffle(numeric_features)  # Reorder the list randomly\n",
    "    numeric_data = X[numeric_features].copy()\n",
    "    corr = numeric_data.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape, dtype=bool), k=1))\n",
    "    the_drop = [column for column in upper.columns if any(upper[column] >= threshold)]\n",
    "    no_correlated_columns = list(numeric_data.drop(the_drop, axis=1).columns)\n",
    "    \n",
    "    return no_correlated_columns\n",
    "\n",
    "\n",
    "def save_list(X, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for element in X:\n",
    "            file.write(str(element) + '\\n')\n",
    "    print(f\"file {file_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders already exist. Skipping folder creation.\n"
     ]
    }
   ],
   "source": [
    "# Define the folder path\n",
    "folder_path = 'FILES/auto'\n",
    "\n",
    "# Check if the folders already exist\n",
    "if os.path.exists(folder_path):\n",
    "    print(\"Folders already exist. Skipping folder creation.\")\n",
    "else:\n",
    "    # Create the folders\n",
    "    os.makedirs(folder_path)\n",
    "    \n",
    "    # Check if the folders are created\n",
    "    if os.path.exists(folder_path):\n",
    "        print(\"Folders created successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to create folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'data\\custom\\all_waves.csv')\n",
    "target='pahosp1y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns 107\n"
     ]
    }
   ],
   "source": [
    "# These columns were extracted mainly from the documentation of the health domain.\n",
    "selected = ['patoilt', 'pacncrsurg', 'padrinkcr', 'pamomage', 'paheight', 'pashophlp', 'pawthh', 'paarms', 'papaina', 'paosleep', 'paodngr', 'padadage', 'pawalkr', 'patoilethlp', 'parxhrtat', 'palgmusaa', 'padoctor1y', 'pameds', 'parxdiabo', 'parxdiab', 'paoangry', 'pahrtatte', 'paclims', 'pamealhlp', 'pahosp1y', 'papainfr', 'pabreast', 'paarthre', 'parafaany', 'parxarthr', 'paweight', 'pahipcomp', 'pagrossaa', 'padrinkbd', 'pamammog', 'parfaany', 'pamobilaa', 'pacncrradn', 'paprmem', 'pashop', 'paeat', 'pauppermoba', 'pacage', 'pastroklmt', 'pacholst', 'pabmi', 'parorgnz', 'paurina2y', 'pagender', 'papapsm', 'paprost', 'paopace', 'pabedhlp', 'parxhibp', 'paoplot', 'pamhip', 'padiabe', 'pafall', 'paglasses', 'pamoney', 'pahibpe', 'paoalchl', 'pasight', 'pawalkhlp', 'pahrtatlmt', 'padrinkb', 'pacncrothr', 'pawheeze', 'pasit', 'pastoop', 'pachair', 'paarthlmt', 'padress', 'paeathlp', 'pastroke', 'papainlv', 'parechrtatt', 'pabathehlp', 'pabreath_m', 'pafatigue', 'pabed', 'pacncrchem', 'pamoneyhlp', 'parxlung_m', 'palunglmt_m', 'pabath', 'palowermoba', 'pamedhlp', 'pahearaid', 'pameals', 'parxstrok', 'pafallinj', 'pahipe_m', 'parifaany', 'parjudg', 'paswell', 'pacncrmeds', 'pasmoken', 'padresshlp', 'pahigov', 'pahearing', 'palift', 'pasmokev', 'paagey', 'pacancre', 'pararcare', 'parfcaren']\n",
    "print(f'number of columns {len(selected)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[target]) \n",
    "\n",
    "df = df[selected].copy()\n",
    "\n",
    "nulls = df.isnull().sum(axis=1) / len(df.columns)\n",
    "df = df[(df[target] != 0) | (nulls <= 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[target]\n",
    "X = df.drop(target,axis=1)\n",
    "X = fast_fill(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_depth': 28, 'subsample': 0.6559545947005226, 'colsample_bytree': 0.9075616505206274, 'eta': 0.03568753700048324, 'min_child_weight': 2, 'learning_rate': 0.0869908981133205, 'n_estimators': 571}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=dp.split_data(X, y, test_size=0.2, random_state=42)\n",
    "X_train, y_train = dp.apply_smote(X_train, y_train)\n",
    "X_train, X_test = dp.apply_standard_scaler(X_train, X_test)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "_X=feature_importance(model, X, y, 85)\n",
    "_X.append('pahosp1y')\n",
    "print(_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    state = random.randint(0, 100)\n",
    "    folder_path = f\"FILES/auto/{state}\"\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"The folder '{folder_path}' already exists. Generating another random number.\")\n",
    "    else:\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully.\")\n",
    "        break\n",
    "\n",
    "for i in range(10):\n",
    "    while True:\n",
    "        random_number = random.uniform(0.21, 0.24)\n",
    "        __X=drop_correlated_features(X, threshold=random_number)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test=dp.split_data(X[__X], y, test_size=0.2, random_state=state)\n",
    "        X_train, y_train = dp.apply_smote(X_train, y_train)\n",
    "        X_train, X_test = dp.apply_standard_scaler(X_train, X_test)\n",
    "        \n",
    "        if len(__X) < 39:\n",
    "            model = XGBClassifier(**params,n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            if auc >= 0.9:\n",
    "                save_list(__X, f'FILES/auto/{state}/F{len(__X)}_AUC_{round(auc,2)}_Index_{i}.txt')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the content of TXT files\n",
    "corpus = []\n",
    "# Iterate through all folders and TXT files\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                content = f.read()\n",
    "                corpus.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of words\n",
    "tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "# Custom stopwords that will not be considered as stopwords\n",
    "custom_stopwords = []\n",
    "\n",
    "# Applying TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stopwords, lowercase=False)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(doc) for doc in tokenized_corpus])\n",
    "\n",
    "# Obtaining unique terms (words) in the corpus\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique terms and their respective TF-IDF values\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    for j, term in enumerate(feature_names):\n",
    "        tfidf_value = tfidf_matrix[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of words ordered by TF-IDF without repeating words\n",
    "words_tfidf = []\n",
    "word_set = set()\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    for j, term in enumerate(feature_names):\n",
    "        tfidf_value = tfidf_matrix[i, j]\n",
    "        if tfidf_value > 0 and term not in word_set:\n",
    "            words_tfidf.append((term, tfidf_value))\n",
    "            word_set.add(term)\n",
    "\n",
    "# Sorting the list by TF-IDF value\n",
    "words_tfidf.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Saving the sorted list of words to a text file\n",
    "with open(\"FILES/tfidf.txt\", \"w\") as f:\n",
    "    for word, tfidf in words_tfidf:\n",
    "        f.write(f\"{word}: {tfidf}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most important features were obtained.\n",
    "- Correlated columns were dropped.\n",
    "- Columns that yielded an AUC of 90 or higher were saved.\n",
    "- TF-IDF was calculated for the columns and sorted based on these values.\n",
    "- Columns that repeat more frequently have higher predictive power.\n",
    "- Columns were then selected based on their TF-IDF and whether they could be transformed into questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
