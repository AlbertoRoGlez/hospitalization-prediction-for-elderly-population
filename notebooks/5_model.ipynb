{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\Leo\\OneDrive\\Escritorio\\Final Proyect\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "import __confing\n",
    "__confing.change_to_root_folder()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "from src import value_filler as vs\n",
    "from src import feature_selection as fs\n",
    "from src import data_procces as dp\n",
    "from src import evaluation as ev\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error,roc_auc_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, model.predict(X_train))\n",
    "    # Validation ROC AUC Score\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc_auc_val = roc_auc_score(y_test, y_pred)\n",
    "    print( f\"Model Accuracy: {acc}\",\n",
    "            f\"Train roc_auc: {roc_auc_train}\", \n",
    "            f\"Test roc_auc: {roc_auc_val}\",\n",
    "            sep='\\n', end='\\n\\n')\n",
    "    print(confusion_matrix(y_test, y_pred),\n",
    "    classification_report(y_test, y_pred),\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Data process`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "df = pd.read_csv(r'data\\waves\\all_waves.csv')\n",
    "target = 'rahosp1y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop\n",
    "df = df.drop(['rahspnit1y', 'wave', 'rahhidnp'], axis=1)\n",
    "\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "porcentaje_nulos = df.isnull().sum(axis=1) / len(df.columns)\n",
    "df = df[(df['rahosp1y'] != 0) | (porcentaje_nulos <= 0.4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling\n",
    "df = fs.fast_fill(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_14896\\1131890659.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_f[target] = df[target]\n"
     ]
    }
   ],
   "source": [
    "# filtering\n",
    "l1 = fs.get_corr_columns(df, target, True)\n",
    "l2 = fs.get_corr_columns(df, target, False)\n",
    "l3 = l1+l2\n",
    "\n",
    "df_f = df[l3]\n",
    "df_f[target] = df[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split --\n",
    "y = df_f['rahosp1y']\n",
    "X = df_f.drop('rahosp1y', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = dp.split_data(\n",
    "    X, y, test_size=0.2, random_state=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample quitado*\n",
    "X_train, y_train = dp.apply_resample(X_train, y_train,v=1.6)\n",
    "X_test, y_test = dp.apply_resample(X_test, y_test,v=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote\n",
    "X_train, y_train = dp.apply_smote(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "X_train, X_test = dp.apply_standard_scaler(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power transform\n",
    "X_train, X_test = dp.apply_power_transform(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"stoped\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Models`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"comment this line to run the cell\")\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.1)\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\n",
    "    random_state = trial.suggest_int('random_state', 0, 100)\n",
    "\n",
    "    base_estimator = DecisionTreeClassifier(\n",
    "        max_depth=1, random_state=random_state)\n",
    "    model = AdaBoostClassifier(\n",
    "        base_estimator=base_estimator,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        algorithm=algorithm,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor precisión encontrada: 0.9101548812110168\n",
      "Mejores hiperparámetros encontrados: {'n_estimators': 123, 'learning_rate': 0.024457369594744295, 'algorithm': 'SAMME', 'random_state': 88}\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los mejores resultados\n",
    "print('Mejor precisión encontrada:', study.best_value)\n",
    "print('Mejores hiperparámetros encontrados:', study.best_params)\n",
    "adaboost_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_params ={\n",
    "    'n_estimators': 123, \n",
    "    'learning_rate': 0.024457369594744295, \n",
    "    'algorithm': 'SAMME', \n",
    "    'random_state': 88\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"comment this line to run the cell\")\n",
    "\n",
    "def objective(trial):\n",
    "    # Definir los espacios de búsqueda para los hiperparámetros de CatBoost\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 200),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.00001, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.1, 10.0),\n",
    "        'random_seed': trial.suggest_int('random_seed', 1, 100),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1.0),\n",
    "        'loss_function': trial.suggest_categorical('loss_function', ['Logloss', 'CrossEntropy']),\n",
    "        'eval_metric': trial.suggest_categorical('eval_metric', ['Accuracy', 'F1', 'AUC']),\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 5, 20),\n",
    "    }\n",
    "\n",
    "    # Crear el clasificador CatBoost con los hiperparámetros sugeridos\n",
    "    model = CatBoostClassifier(**params)\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    # Calcular las predicciones en el conjunto de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calcular la precisión del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Crear el estudio de Optuna y optimizar los hiperparámetros\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n",
    "\n",
    "# Obtener los mejores resultados\n",
    "best_value = study.best_value\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor precisión encontrada: 0.9044081575443269\n",
      "Mejores parámetros encontrados: {'iterations': 187, 'learning_rate': 0.040676323292287556, 'depth': 7, 'l2_leaf_reg': 2.9831156146703663, 'random_seed': 66, 'bootstrap_type': 'Bernoulli', 'subsample': 0.3795675559951529, 'colsample_bylevel': 0.8542347504139569, 'loss_function': 'Logloss', 'eval_metric': 'F1', 'early_stopping_rounds': 14}\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los mejores resultados\n",
    "print('Mejor precisión encontrada:', study.best_value)\n",
    "print('Mejores parámetros encontrados:', study.best_params)\n",
    "catboost_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7270255573681349\n",
      "Train roc_auc: 0.7453974895397489\n",
      "Test roc_auc: 0.7270255573681349\n",
      "\n",
      "[[1687  152]\n",
      " [ 852  987]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.92      0.77      1839\n",
      "         1.0       0.87      0.54      0.66      1839\n",
      "\n",
      "    accuracy                           0.73      3678\n",
      "   macro avg       0.77      0.73      0.72      3678\n",
      "weighted avg       0.77      0.73      0.72      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catboost_params = {\n",
    "    'iterations': 187, \n",
    "    'learning_rate': 0.040676323292287556, \n",
    "    'depth': 7, \n",
    "    'l2_leaf_reg': 2.9831156146703663, \n",
    "    'random_seed': 66, \n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.3795675559951529, \n",
    "    'colsample_bylevel': 0.8542347504139569, \n",
    "    'loss_function': 'Logloss', \n",
    "    'eval_metric': 'F1', \n",
    "    'early_stopping_rounds': 14\n",
    "}\n",
    "model = CatBoostClassifier(**catboost_params)\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "get_performance(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Model Accuracy: 0.9044081575443269\n",
    "# Train roc_auc: 0.8338428430642987\n",
    "# Test roc_auc: 0.7009613815616501"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"comment this line to run the cell\")\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'rf']),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.00001, 0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'subsample_for_bin': trial.suggest_int('subsample_for_bin', 100000, 300000),\n",
    "        'objective': 'binary',  \n",
    "        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0., 1.),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 30),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'subsample_freq': trial.suggest_int('subsample_freq', 0, 5),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0., 1.),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0., 1.),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 100),\n",
    "        'importance_type': trial.suggest_categorical('importance_type', ['split', 'gain']),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 1.0),  \n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),  \n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Return accuracy for maximization\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor precisión encontrada: 0.9101548812110168\n",
      "Mejores parámetros encontrados: {'boosting_type': 'rf', 'num_leaves': 61, 'max_depth': 1, 'learning_rate': 0.01816560652901298, 'n_estimators': 87, 'subsample_for_bin': 290089, 'class_weight': 'balanced', 'min_split_gain': 0.4341050061211783, 'min_child_weight': 0.30754031398986437, 'min_child_samples': 11, 'subsample': 0.9812553236546777, 'subsample_freq': 4, 'colsample_bytree': 0.8148632932127071, 'reg_alpha': 0.9556694136778114, 'reg_lambda': 0.49691043770864046, 'random_state': 53, 'importance_type': 'split', 'bagging_fraction': 0.3099214263868336, 'bagging_freq': 7}\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los mejores resultados\n",
    "print('Mejor precisión encontrada:', study.best_value)\n",
    "print('Mejores parámetros encontrados:', study.best_params)\n",
    "\n",
    "lgbm_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.3099214263868336, subsample=0.9812553236546777 will be ignored. Current value: bagging_fraction=0.3099214263868336\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=4 will be ignored. Current value: bagging_freq=7\n",
      "Model Accuracy: 0.6514410005437737\n",
      "Train roc_auc: 0.6577928870292887\n",
      "Test roc_auc: 0.6514410005437739\n",
      "\n",
      "[[1839    0]\n",
      " [1282  557]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      1.00      0.74      1839\n",
      "         1.0       1.00      0.30      0.46      1839\n",
      "\n",
      "    accuracy                           0.65      3678\n",
      "   macro avg       0.79      0.65      0.60      3678\n",
      "weighted avg       0.79      0.65      0.60      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_params = {\n",
    "    'boosting_type': 'rf', \n",
    "    'num_leaves': 61, \n",
    "    'max_depth': 1, \n",
    "    'learning_rate': 0.01816560652901298, \n",
    "    'n_estimators': 87, \n",
    "    'subsample_for_bin': 290089, \n",
    "    'class_weight': 'balanced', \n",
    "    'min_split_gain': 0.4341050061211783, \n",
    "    'min_child_weight': 0.30754031398986437, \n",
    "    'min_child_samples': 11, \n",
    "    'subsample': 0.9812553236546777, \n",
    "    'subsample_freq': 4, \n",
    "    'colsample_bytree': 0.8148632932127071, \n",
    "    'reg_alpha': 0.9556694136778114, \n",
    "    'reg_lambda': 0.49691043770864046, \n",
    "    'random_state': 53, \n",
    "    'importance_type': 'split', \n",
    "    'bagging_fraction': 0.3099214263868336, \n",
    "    'bagging_freq': 7\n",
    "} # 95 auc\n",
    "\n",
    "model = lgb.LGBMClassifier(**lgbm_params)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "get_performance(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"comment this line to run the cell\")\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'eta': trial.suggest_loguniform('eta', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'use_label_encoder': False\n",
    "    }\n",
    "    # Fit the model\n",
    "    optuna_model = XGBClassifier(**params)\n",
    "    optuna_model.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = optuna_model.predict(X_test)\n",
    "    # Evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Create the Optuna study and optimize the parameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor precisión encontrada: 0.9095241432475997\n",
      "Mejores parámetros encontrados: {'booster': 'gbtree', 'eta': 0.012093142041979083, 'max_depth': 6, 'subsample': 0.8874936946322917, 'learning_rate': 0.14552872398461472, 'n_estimators': 245, 'min_child_weight': 5, 'gamma': 5.264572425726908e-05, 'colsample_bytree': 0.3424548012820728, 'reg_alpha': 1.3334029646882244e-06, 'reg_lambda': 0.0006323060575623945}\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los mejores resultados\n",
    "print('Mejor precisión encontrada:', study.best_value)\n",
    "print('Mejores parámetros encontrados:', study.best_params)\n",
    "\n",
    "xgboost_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7183251767264818\n",
      "Train roc_auc: 0.7980622131565527\n",
      "Test roc_auc: 0.7183251767264818\n",
      "\n",
      "[[1686  153]\n",
      " [ 883  956]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.92      0.76      1839\n",
      "         1.0       0.86      0.52      0.65      1839\n",
      "\n",
      "    accuracy                           0.72      3678\n",
      "   macro avg       0.76      0.72      0.71      3678\n",
      "weighted avg       0.76      0.72      0.71      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_params ={\n",
    "    'booster': 'gbtree', \n",
    "    'eta': 0.012093142041979083, \n",
    "    'max_depth': 6, \n",
    "    'subsample': 0.8874936946322917, \n",
    "    'learning_rate': 0.14552872398461472, \n",
    "    'n_estimators': 245, \n",
    "    'min_child_weight': 5, \n",
    "    'gamma': 5.264572425726908e-05, \n",
    "    'colsample_bytree': 0.3424548012820728, \n",
    "    'reg_alpha': 1.3334029646882244e-06, \n",
    "    'reg_lambda': 0.0006323060575623945\n",
    "} # 86 auc\n",
    "\n",
    "model = XGBClassifier(**xgboost_params)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "get_performance(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    \n",
    "    model = GradientBoostingClassifier(**params)\n",
    "    with parallel_backend('multiprocessing'):\n",
    "        model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mejor precisión encontrada:', study.best_value)\n",
    "print('Mejores parámetros encontrados:', study.best_params)\n",
    "stochastic_params = study.best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    catboost_params = {\n",
    "        'iterations': trial.suggest_int('catboost_iterations', 50, 200),\n",
    "        'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('catboost_depth', 3, 10),\n",
    "        # Otros parámetros de CatBoost\n",
    "    }\n",
    "    lgbm_params = {\n",
    "        'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': trial.suggest_int('lgbm_n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('lgbm_max_depth', 1, 10),\n",
    "        # Otros parámetros de LightGBM\n",
    "    }\n",
    "    xgboost_params = {\n",
    "        'learning_rate': trial.suggest_loguniform('xgboost_learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': trial.suggest_int('xgboost_n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('xgboost_max_depth', 1, 10),\n",
    "        # Otros parámetros de XGBoost\n",
    "    }\n",
    "    \n",
    "    catboost_model = CatBoostClassifier(**catboost_params)\n",
    "    lgbm_model = LGBMClassifier(**lgbm_params)\n",
    "    xgboost_model = XGBClassifier(**xgboost_params)\n",
    "    \n",
    "    estimators = [\n",
    "        ('catboost', catboost_model),\n",
    "        ('lgbm', lgbm_model),\n",
    "        ('xgboost', xgboost_model)\n",
    "    ]\n",
    "    \n",
    "    voting = VotingClassifier(estimators, voting='soft')\n",
    "    voting.fit(X_train, y_train)\n",
    "    y_pred = voting.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Retornar la precisión para maximizar\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n",
    "\n",
    "best_params = study.best_params\n",
    "catboost_params = {k[10:]: v for k,v in best_params.items() if k.startswith('catboost_')}\n",
    "lgbm_params = {k[5:]: v for k,v in best_params.items() if k.startswith('lgbm_')}\n",
    "xgboost_params = {k[8:]: v for k,v in best_params.items() if k.startswith('xgboost_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best CatBoost params:\", catboost_params)\n",
    "print(\"Best LightGBM params:\", lgbm_params)\n",
    "print(\"Best XGBoost params:\", xgboost_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
